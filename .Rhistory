})
gamma <- min(gamma_vals)
# Update the residuals and beta as per your notes
R <- R - gamma * u
for (j in A) {
s_j <- sign(C[j])
beta[j] <- beta[j] + gamma * s_j * (ginv(t(X_A) %*% X_A) %*% t(X_A) %*% u)[which(A == j)]
}
# Stop if residuals are small
if (sum(R^2) < 1e-8) {
break
}
}
return(beta)
}
testfunc(X,y)
# LAR Function
testfunc <- function(X, Y, max_iter = NULL) {
# Standardize predictors
X <- scale(X)
Y <- scale(Y, center = TRUE, scale = FALSE)
n <- nrow(X)
p <- ncol(X)
# Initialize variables
R <- Y - mean(Y)
A <- c()
beta <- rep(0, p)
if (is.null(max_iter)) {
max_iter <- min(n - 1, p)
}
for (k in 1:max_iter) {
# (2) Find the predictor most correlated with the current residual
C <- colSums(t(X) %*% R) # Correlations
j_star <- which.max(abs(C))
C_max <- C[j_star]
A <- c(A, j_star)
X_A <- X[, A, drop = FALSE]
# (3) Update the coefficients
u <- X_A %*% solve(t(X_A) %*% X_A) %*% sign(C[A])
# Calculate step size gamma
A_k <- crossprod(X_A, u)
gamma_vals <- sapply(setdiff(1:p, A), function(j) {
C_j <- C[j]
A_j <- t(X[, j]) %*% u
return(min((C_max - C_j) / (1/sqrt(A_k^2) - A_j), (C_max + C_j) / (1/sqrt(A_k^2) + A_j)))
})
gamma <- min(gamma_vals)
# Update the residuals and beta as per your notes
R <- R - gamma * u
for (j in A) {
s_j <- sign(C[j])
beta[j] <- beta[j] + gamma * s_j * (ginv(t(X_A) %*% X_A) %*% t(X_A) %*% u)[which(A == j)]
}
# Stop if residuals are small
if (sum(R^2) < 1e-8) {
break
}
}
return(beta)
}
testfunc(X,y)
# LAR Function
testfunc <- function(X, Y, max_iter = NULL) {
# Standardize predictors
X <- scale(X)
Y <- scale(Y, center = TRUE, scale = FALSE)
n <- nrow(X)
p <- ncol(X)
# Initialize variables
R <- Y - mean(Y)
A <- c()
beta <- rep(0, p)
if (is.null(max_iter)) {
max_iter <- min(n - 1, p)
}
for (k in 1:max_iter) {
# (2) Find the predictor most correlated with the current residual
C <- colSums(t(X) %*% R) # Correlations
j_star <- which.max(abs(C))
C_max <- C[j_star]
A <- c(A, j_star)
X_A <- X[, A, drop = FALSE]
# (3) Update the coefficients
u <- X_A %*% solve(t(X_A) %*% X_A) %*% sign(C[A])
# Calculate step size gamma
A_k <- crossprod(X_A, u)
gamma_vals <- sapply(setdiff(1:p, A), function(j) {
C_j <- C[j]
A_j <- t(X[, j]) %*% u
return(min((C_max - C_j) / (1/sqrt(A_k^2) - A_j), (C_max + C_j) / (1/sqrt(A_k^2) + A_j)))
})
gamma <- min(gamma_vals)
# Update the residuals and beta as per your notes
R <- R - gamma * u
for (j in A) {
s_j <- sign(C[j])
beta[j] <- beta[j] + gamma * s_j * (solve(t(X_A) %*% X_A) %*% t(X_A) %*% u)[which(A == j)]
}
# Stop if residuals are small
if (sum(R^2) < 1e-8) {
break
}
}
return(beta)
}
# Initialize variables
R <- Y - mean(Y)
A <- c()
beta <- rep(0, p)
if (is.null(max_iter)) {
max_iter <- min(n - 1, p)
}
testfunc(X,y)
# LAR Function
testfunc <- function(X, Y, max_iter = NULL) {
# Standardize predictors
X <- scale(X)
Y <- scale(Y, center = TRUE, scale = FALSE)
n <- nrow(X)
p <- ncol(X)
# Initialize variables
R <- Y - mean(Y)
A <- c()
beta <- rep(0, p)
if (is.null(max_iter)) {
max_iter <- min(n - 1, p)
}
for (k in 1:max_iter) {
# (2) Find the predictor most correlated with the current residual
C <- colSums(t(X) %*% R) # Correlations
j_star <- which.max(abs(C))
C_max <- C[j_star]
A <- c(A, j_star)
X_A <- X[, A, drop = FALSE]
# (3) Update the coefficients
u <- X_A %*% solve(t(X_A) %*% X_A) %*% sign(C[A])
# Calculate step size gamma
A_k <- crossprod(X_A, u)
gamma_vals <- sapply(setdiff(1:p, A), function(j) {
C_j <- C[j]
A_j <- t(X[, j]) %*% u
return(min((C_max - C_j) / (1/sqrt(A_k^2) - A_j), (C_max + C_j) / (1/sqrt(A_k^2) + A_j)))
})
gamma <- min(gamma_vals)
# Update the residuals and beta as per your notes
R <- R - gamma * u
for (j in A) {
s_j <- sign(C[j])
beta[j] <- beta[j] + gamma * s_j * (solve(t(X_A) %*% X_A) %*% t(X_A) %*% u)[which(A == j)]
}
}
return(beta)
}
testfunc(X,y)
C
test <- c(1,2,-2)
sign(test)
# Initialize
n <- nrow(X)
p <- ncol(X)
if (is.null(iter)) {
# We arrive at the least squares solution after min(n-1, p) steps
iter <- min(n - 1, p)
}
# Standardize regressors and initialize the first residuum
X_scaled <- scale(X)
# pre calculate X_scaled for greater efficiency
X_scaled_transposed <- t(X_scaled)
Xt_X_prod_inverse <- solve(X_scaled_transposed %*% X_scaled)
r <- y - mean(y)
coefficient_matrix <- matrix(NA, nrow = iter, ncol = p)
# Helper
active_variables <- logical(p)
# We arrive at the least squares solution after min(n-1, p) steps
iter <- min(n - 1, p)
# Standardize regressors and initialize the first residuum
X_scaled <- scale(X)
# pre calculate X_scaled for greater efficiency
X_scaled_transposed <- t(X_scaled)
Xt_X_prod_inverse <- solve(X_scaled_transposed %*% X_scaled)
r <- y - mean(y)
coefficient_matrix <- matrix(NA, nrow = iter, ncol = p)
# Helper
active_variables <- logical(p)
beta <- double(p)
# Find the regressor x most correlated with current r
C <- t(X) %*% r
j_star <- which.max(abs(r_cor))
C_max <- c[j_star]
# update the active set (drop = FALSE in order to keep the class of X)
active_variables[j_star] <- TRUE
C_max <- c[j_star]
C
# Find the regressor x most correlated with current r
C <- t(X) %*% r
j_star <- which.max(abs(C))
C_max <- c[j_star]
C_max <- C[j_star]
# update the active set (drop = FALSE in order to keep the class of X)
active_variables[j_star] <- TRUE
A <- X_scaled[, active_variables, drop = FALSE]
# stepsize alpha
X_tilde <- sign(C) * A
XTX_inv <- solve(t(X_tilde) %*% X_tilde)
sign(C)
A
# stepsize alpha
X_tilde <- sign(C) %*% A
A %*% sign(C)
dim(A)
dim(sign(c))
dim(sign(C))
# stepsize alpha
X_tilde <- sign(C[active_variables]) %*% A
# stepsize alpha
X_tilde <- sign(C[active_variables]) %*% A
dim(sign(C[active_variables]))
active_variables
C[active_variables]
sign(C[active_variables])
# stepsize alpha
X_tilde <- sign(C[active_variables]) * A
XTX_inv <- solve(t(X_tilde) %*% X_tilde)
ones <- matrix(1, ncol(A), 1)
ones_tranpose <- t(ones)
# calculate w
w <- ones_tranpose %*% XTX_inv %*% ones
sqrt_w <- as.double(sqrt(w))
u <- (X_tilde %*% XTX_inv &*& ones) / sqrt_w
u <- (X_tilde %*% XTX_inv %*% ones) / sqrt_w
B <- t(X_tilde) %*% u
# Take the min
alpha_pos <- ((C_max - C) / ((1 / sqrt_w) - B))
(C_max - C
)
((1 / sqrt_w) - B)
# Take the min
alpha_pos <- ((C_max - C) / ((1 / sqrt_w) - B))
(C_max - C) / ((1 / sqrt_w) - B)
((1 / sqrt_w) - B)
(C_max - C)
(C_max - C)/2
(C_max - C) / (1/sqrt_w)
(C_max - C) / (1/sqrt_w) - B
(C_max - C) / ((1/sqrt_w) - B)
B
(C_max-C) / B
dim(B)
(C_max-C) / as.double(B)
as.double((C_max-C) / as.double(B))
B <- as.double(t(X_tilde) %*% u)
# Take the min
alpha_pos <- ((C_max - C) / ((1 / sqrt_w) - B))
alpha_neg <- ((C_max + C) / ((1 / sqrt_w) + B))
alpha <- min(c(alpha_pos, alpha_neg))
alpha
# Calculate the current model and update beta
delta_step <- solve(t(A) %*% A, t(A) %*% r)
beta[active_variables] <- beta[active_variables] + alpha * delta_step
delta_step
# Calculate the current model and update beta
for (j in active_variables) {
beta[j] = beta[j] + alpha * sign(C[j])* solve(t(A) %*% A, t(A) %*% u)[j]
}
beta
least_angle_regression <- function(X, y, iter = NULL, verbose = T) {
# Initialize
n <- nrow(X)
p <- ncol(X)
if (is.null(iter)) {
# We arrive at the least squares solution after min(n-1, p) steps
iter <- min(n - 1, p)
}
# Standardize regressors and initialize the first residuum
X_scaled <- scale(X)
# pre calculate X_scaled for greater efficiency
X_scaled_transposed <- t(X_scaled)
Xt_X_prod_inverse <- solve(X_scaled_transposed %*% X_scaled)
r <- y - mean(y)
coefficient_matrix <- matrix(NA, nrow = iter, ncol = p)
# Helper
active_variables <- logical(p)
beta <- double(p)
for (i in 1:iter) {
# Find the regressor x most correlated with current r
C <- t(X) %*% r
j_star <- which.max(abs(C))
C_max <- C[j_star]
# update the active set (drop = FALSE in order to keep the class of X)
active_variables[j_star] <- TRUE
A <- X_scaled[, active_variables, drop = FALSE]
# stepsize alpha
X_tilde <- sign(C[active_variables]) * A
XTX_inv <- solve(t(X_tilde) %*% X_tilde)
ones <- matrix(1, ncol(A), 1)
ones_tranpose <- t(ones)
# calculate w
w <- ones_tranpose %*% XTX_inv %*% ones
sqrt_w <- as.double(sqrt(w))
u <- (X_tilde %*% XTX_inv %*% ones) / sqrt_w
B <- as.double(t(X_tilde) %*% u)
# Take the min
alpha_pos <- ((C_max - C) / ((1 / sqrt_w) - B))
alpha_neg <- ((C_max + C) / ((1 / sqrt_w) + B))
alpha <- min(c(alpha_pos, alpha_neg))
r <- r - A %*% delta_step
# Calculate the current model and update beta
for (j in active_variables) {
beta[j] = beta[j] + alpha * sign(C[j])* solve(t(A) %*% A, t(A) %*% u)[j]
}
# update the residual
#r <- y - A %*% as.matrix(beta[active_variables])
#
############################################################################
# Verbose option
if (verbose) {
cat("Iteration:", i, "\n")
cat("Active Variables:", which(active_variables), "\n")
cat("delta :", delta_step, "\n")
cat("alpha :", alpha, "\n")
cat("Coefficients:", beta, "\n")
cat("\n")
}
# update output data
coefficient_matrix[i, ] <- beta
}
# Calculate arc length
arg_length <- apply(coefficient_matrix, 1, function(x) {sum(abs(x))})
# Calculate R-Squared for each model
r2 <- apply(coefficient_matrix, 1, function(beta) {calculate_R2(y, X %*% beta)}
)
output_list <- list(coefficients = coefficient_matrix,
l1_arc_length = arg_length,
R2 = r2)
# Modify S3 class
class(output_list) <- "LAR"
return (output_list)
}
least_angle_regression(X,y)
logical(2)
#' Calculate the LARS path
#'
#' @param X data frame containing the covariates
#' @param Y response vector
#'
#' @return list ..... TODO
### TODO ####
# plot(...) for plotting L1-arc length
# step output
# Lasso option !!!!
# How to select the "best" model? CV? or just provide all models an the user can decide ?
# Include stepsize alpha !!!!!!!!!
least_angle_regression <- function(X, y, iter = NULL, verbose = T) {
# Initialize
n <- nrow(X)
p <- ncol(X)
if (is.null(iter)) {
# We arrive at the least squares solution after min(n-1, p) steps
iter <- min(n - 1, p)
}
# Standardize regressors and initialize the first residuum
X_scaled <- scale(X)
# pre calculate X_scaled for greater efficiency
X_scaled_transposed <- t(X_scaled)
Xt_X_prod_inverse <- solve(X_scaled_transposed %*% X_scaled)
r <- y - mean(y)
coefficient_matrix <- matrix(NA, nrow = iter, ncol = p)
# Helper
active_variables <- logical(p)
beta <- double(p)
for (i in 1:iter) {
# Find the regressor x most correlated with current r
C <- t(X) %*% r
j_star <- which.max(abs(C))
C_max <- C[j_star]
# update the active set (drop = FALSE in order to keep the class of X)
active_variables[j_star] <- TRUE
A <- X_scaled[, active_variables, drop = FALSE]
# stepsize alpha
X_tilde <- sign(C[active_variables]) * A
XTX_inv <- solve(t(X_tilde) %*% X_tilde)
ones <- matrix(1, ncol(A), 1)
ones_tranpose <- t(ones)
# calculate w
w <- ones_tranpose %*% XTX_inv %*% ones
sqrt_w <- as.double(sqrt(w))
u <- (X_tilde %*% XTX_inv %*% ones) / sqrt_w
B <- as.double(t(X_tilde) %*% u)
# Take the min
alpha_pos <- ((C_max - C) / ((1 / sqrt_w) - as.double(B)))
alpha_neg <- ((C_max + C) / ((1 / sqrt_w) + as.double(B)))
alpha <- min(c(alpha_pos, alpha_neg))
r <- r - A %*% delta_step
# Calculate the current model and update beta
for (j in active_variables) {
beta[j] = beta[j] + alpha * sign(C[j])* solve(t(A) %*% A, t(A) %*% u)[j]
}
# update the residual
#r <- y - A %*% as.matrix(beta[active_variables])
#
############################################################################
# Verbose option
if (verbose) {
cat("Iteration:", i, "\n")
cat("Active Variables:", which(active_variables), "\n")
cat("delta :", delta_step, "\n")
cat("alpha :", alpha, "\n")
cat("Coefficients:", beta, "\n")
cat("\n")
}
# update output data
coefficient_matrix[i, ] <- beta
}
# Calculate arc length
arg_length <- apply(coefficient_matrix, 1, function(x) {sum(abs(x))})
# Calculate R-Squared for each model
r2 <- apply(coefficient_matrix, 1, function(beta) {calculate_R2(y, X %*% beta)}
)
output_list <- list(coefficients = coefficient_matrix,
l1_arc_length = arg_length,
R2 = r2)
# Modify S3 class
class(output_list) <- "LAR"
return (output_list)
}
least_angle_regression(X,y)
#' Calculate the LARS path
#'
#' @param X data frame containing the covariates
#' @param Y response vector
#'
#' @return list ..... TODO
### TODO ####
# plot(...) for plotting L1-arc length
# step output
# Lasso option !!!!
# How to select the "best" model? CV? or just provide all models an the user can decide ?
# Include stepsize alpha !!!!!!!!!
least_angle_regression <- function(X, y, iter = NULL, verbose = T) {
# Initialize
n <- nrow(X)
p <- ncol(X)
if (is.null(iter)) {
# We arrive at the least squares solution after min(n-1, p) steps
iter <- min(n - 1, p)
}
# Standardize regressors and initialize the first residuum
X_scaled <- scale(X)
# pre calculate X_scaled for greater efficiency
X_scaled_transposed <- t(X_scaled)
Xt_X_prod_inverse <- solve(X_scaled_transposed %*% X_scaled)
r <- y - mean(y)
coefficient_matrix <- matrix(NA, nrow = iter, ncol = p)
# Helper
active_variables <- logical(p)
beta <- double(p)
for (i in 1:iter) {
# Find the regressor x most correlated with current r
C <- t(X) %*% r
j_star <- which.max(abs(C))
C_max <- C[j_star]
# update the active set (drop = FALSE in order to keep the class of X)
active_variables[j_star] <- TRUE
A <- X_scaled[, active_variables, drop = FALSE]
# stepsize alpha
X_tilde <- sign(C[active_variables]) * A
XTX_inv <- solve(t(X_tilde) %*% X_tilde)
ones <- matrix(1, ncol(A), 1)
ones_tranpose <- t(ones)
# calculate w
w <- ones_tranpose %*% XTX_inv %*% ones
sqrt_w <- as.double(sqrt(w))
u <- (X_tilde %*% XTX_inv %*% ones) / sqrt_w
B <- as.double(t(X_tilde) %*% u)
# Take the min
alpha_pos <- ((C_max - C) / ((1 / sqrt_w) - as.double(B)))
alpha_neg <- ((C_max + C) / ((1 / sqrt_w) + as.double(B)))
alpha <- min(c(alpha_pos, alpha_neg))
r <- r - alpha*u
# Calculate the current model and update beta
for (j in active_variables) {
beta[j] = beta[j] + alpha * sign(C[j])* solve(t(A) %*% A, t(A) %*% u)[j]
}
# update the residual
#r <- y - A %*% as.matrix(beta[active_variables])
#
############################################################################
# Verbose option
if (verbose) {
cat("Iteration:", i, "\n")
cat("Active Variables:", which(active_variables), "\n")
cat("delta :", delta_step, "\n")
cat("alpha :", alpha, "\n")
cat("Coefficients:", beta, "\n")
cat("\n")
}
# update output data
coefficient_matrix[i, ] <- beta
}
# Calculate arc length
arg_length <- apply(coefficient_matrix, 1, function(x) {sum(abs(x))})
# Calculate R-Squared for each model
r2 <- apply(coefficient_matrix, 1, function(beta) {calculate_R2(y, X %*% beta)}
)
output_list <- list(coefficients = coefficient_matrix,
l1_arc_length = arg_length,
R2 = r2)
# Modify S3 class
class(output_list) <- "LAR"
return (output_list)
}
least_angle_regression(X,y)
